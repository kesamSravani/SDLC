# -*- coding: utf-8 -*-
"""sdlc

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1iHiq0GOqqs1hJShOKXQzRC8YLoZ7xcar
"""

# Install required packages
!pip install streamlit pyngrok PyMuPDF requests python-dotenv
!pip install -U transformers torch streamlit

# --- 1. Create SmartSDLC app with IBM Granite ---
print("Creating SmartSDLC app with IBM Granite model...")

# Create the app file
with open('smartsdlc_app.py', 'w', encoding='utf-8') as f:
    f.write('''import streamlit as st
import fitz  # PyMuPDF
import time
import warnings
import os
from transformers import AutoTokenizer, AutoModelForCausalLM
import torch
warnings.filterwarnings("ignore")

# IBM Granite Model Configuration
@st.cache_resource
def load_ibm_model():
    """Load IBM Granite model with caching for performance"""
    try:
        print("Loading IBM Granite model...")
        tokenizer = AutoTokenizer.from_pretrained("ibm-granite/granite-3.3-2b-instruct", trust_remote_code=True)
        model = AutoModelForCausalLM.from_pretrained("ibm-granite/granite-3.3-2b-instruct", trust_remote_code=True)

        # Use GPU if available
        if torch.cuda.is_available():
            model = model.to("cuda")
            print("âœ… Model loaded on GPU")
        else:
            print("âœ… Model loaded on CPU")

        return tokenizer, model
    except Exception as e:
        st.error(f"Error loading model: {e}")
        return None, None

def call_ibm_model(prompt, max_tokens=500, temperature=0.7):
    """Call IBM Granite model directly"""
    try:
        tokenizer, model = load_ibm_model()
        if tokenizer is None or model is None:
            return "âŒ Model not loaded properly"

        # Format prompt for IBM Granite
        formatted_prompt = f"<|user|>\\n{prompt}\\n<|assistant|>\\n"

        # Tokenize input
        inputs = tokenizer(formatted_prompt, return_tensors="pt", truncation=True, max_length=512)

        # Move to GPU if available
        if torch.cuda.is_available():
            inputs = {k: v.to("cuda") for k, v in inputs.items()}

        # Generate response
        with torch.no_grad():
            outputs = model.generate(
                **inputs,
                max_new_tokens=max_tokens,
                temperature=temperature,
                do_sample=True,
                pad_token_id=tokenizer.eos_token_id,
                eos_token_id=tokenizer.eos_token_id
            )

        # Decode response
        response = tokenizer.decode(outputs[0], skip_special_tokens=True)

        # Extract only the assistant response
        if "<|assistant|>" in response:
            response = response.split("<|assistant|>")[-1].strip()

        return response if response else "No response generated"

    except Exception as e:
        return f"âŒ Error: {str(e)}"

def ask_model(prompt, max_tokens=400):
    """Ask IBM Granite model with system context"""
    system_prompt = "You are IBM's AI assistant. Provide concise, enterprise-focused solutions with clear business value."
    full_prompt = f"{system_prompt}\\n\\nUser: {prompt}\\nAssistant:"
    return call_ibm_model(full_prompt, max_tokens)

def analyze_requirements(text):
    """Analyze requirements from text and extract key software requirements"""
    prompt = f"""Analyze this text using IBM's enterprise methodology:

{text[:2000]}

Provide:
**Functional Requirements:** [List with business focus]
**Non-Functional Requirements:** [Scalability, security, performance]
**Technical Requirements:** [IBM enterprise standards]
**User Stories:** [As a [user], I want [feature] so that [business benefit]]

Keep it concise and enterprise-focused."""

    return ask_model(prompt, 600)

def generate_code(prompt, language, framework):
    """Generate code based on prompt, language, and framework"""
    framework_text = f" with {framework}" if framework != "None" else " (vanilla)"
    code_prompt = f"""Generate enterprise-grade {language} code{framework_text} for: {prompt}

Provide:
1. Clean, working code with IBM standards
2. Brief comments explaining business logic
3. Error handling for enterprise use
4. Setup instructions

Keep it concise and production-ready."""

    return ask_model(code_prompt, 800)

def generate_tests(code, test_type, language):
    """Generate test cases from code or requirements"""
    test_prompt = f"""Generate {test_type} tests for this {language} code:

```{language}
{code}
```

Provide:
1. Complete test suite with IBM standards
2. Cover normal cases, edge cases, security
3. Clear test descriptions
4. Setup for enterprise CI/CD

Keep it concise and enterprise-ready."""

    return ask_model(test_prompt, 600)

def chat_assistant(message, context=""):
    """AI assistant for SDLC and development questions"""
    full_prompt = f"""You are IBM's AI assistant. Provide concise, enterprise-focused advice on software development, SDLC, and engineering best practices.

Context: {context}

Question: {message}

Assistant:"""

    return call_ibm_model(full_prompt, 500)

# Streamlit UI Configuration
st.set_page_config(page_title="SmartSDLC - IBM Granite", page_icon="ğŸš€", layout="wide")
st.title("ğŸš€ SmartSDLC - Comprehensive AI Development Tool")
st.markdown("*Powered by IBM Model - All-in-One Software Development Assistant*")

# Sidebar for navigation
st.sidebar.title("ğŸ› ï¸ SmartSDLC")
st.sidebar.markdown("**Choose your tool:**")

# Navigation
page = st.sidebar.selectbox(
    "Select Functionality:",
    ["ğŸ“„ Requirement Analysis", "ğŸ’» AI Code Generator", "ğŸ§ª Test Generator", "ğŸ¤– AI Assistant"]
)

# 1. PDF/Prompt-Based Requirement Analysis
if page == "ğŸ“„ Requirement Analysis":
    st.header("ğŸ“„ Requirement Analysis")
    st.markdown("*Extract and organize software requirements from PDFs or text prompts*")

    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("Input Method")

        # File upload option
        uploaded_file = st.file_uploader("Upload PDF Document", type=['pdf'])

        # Text input option
        requirements_text = st.text_area(
            "Or Enter Requirements Text:",
            placeholder="Enter your project requirements, user stories, or business needs here...",
            height=200
        )

        if st.button("ğŸ” Analyze Requirements", type="primary"):
            text_to_process = ""

            # Process uploaded PDF
            if uploaded_file:
                try:
                    with st.spinner("ğŸ“– Reading PDF..."):
                        pdf_document = fitz.open(stream=uploaded_file.read(), filetype="pdf")
                        for page in pdf_document:
                            text_to_process += page.get_text()
                        pdf_document.close()
                    st.success("âœ… PDF processed successfully!")
                except Exception as e:
                    st.error(f"Error reading PDF: {e}")

            # Process text input
            elif requirements_text:
                text_to_process = requirements_text

            if text_to_process:
                with st.spinner("ğŸ¤– Analyzing requirements with IBM Granite..."):
                    analysis = analyze_requirements(text_to_process)
                    st.session_state.requirement_analysis = analysis
            else:
                st.error("Please provide requirements text or upload a PDF file.")

    with col2:
        st.subheader("Analysis Results")
        if 'requirement_analysis' in st.session_state:
            st.markdown(st.session_state.requirement_analysis)

            # Download button
            st.download_button(
                label="ğŸ“¥ Download Analysis",
                data=st.session_state.requirement_analysis,
                file_name="requirement_analysis.md",
                mime="text/markdown"
            )
        else:
            st.info("Requirement analysis results will appear here after processing.")

# 2. AI Code Generator
elif page == "ğŸ’» AI Code Generator":
    st.header("ğŸ’» AI Code Generator")
    st.markdown("*Generate frontend/backend code from user prompts with language and framework selection*")

    st.subheader("Code Generation Settings")

    # Language selection
    languages = ["Python", "JavaScript", "Java", "C#", "Go", "Rust", "TypeScript", "PHP", "Ruby", "Swift"]
    selected_language = st.selectbox("Select Programming Language:", languages)

    # Framework selection based on language
    frameworks = {
        "Python": ["None", "Django", "Flask", "FastAPI", "Streamlit", "PyTorch", "TensorFlow", "Pandas", "NumPy"],
        "JavaScript": ["None", "React", "Vue.js", "Angular", "Node.js", "Express", "Next.js", "Nuxt.js"],
        "Java": ["None", "Spring Boot", "Spring MVC", "Hibernate", "JUnit", "Maven", "Gradle"],
        "C#": ["None", ".NET Core", "ASP.NET", "Entity Framework", "XUnit", "NUnit"],
        "Go": ["None", "Gin", "Echo", "Fiber", "GORM", "Testify"],
        "Rust": ["None", "Actix", "Rocket", "Warp", "Tokio", "Serde"],
        "TypeScript": ["None", "React", "Vue.js", "Angular", "Node.js", "Express", "NestJS"],
        "PHP": ["None", "Laravel", "Symfony", "CodeIgniter", "WordPress", "Drupal"],
        "Ruby": ["None", "Ruby on Rails", "Sinatra", "RSpec", "Capybara"],
        "Swift": ["None", "UIKit", "SwiftUI", "Core Data", "Combine", "XCTest"]
    }

    selected_framework = st.selectbox("Select Framework:", frameworks.get(selected_language, ["None"]))

    # Code generation prompt
    code_prompt = st.text_area(
        "Describe what you want to build:",
        placeholder="Describe the functionality, features, or code you want to generate...",
        height=150
    )

    if st.button("âš¡ Generate Code", type="primary"):
        if code_prompt:
            with st.spinner(f"ğŸ¤– Generating {selected_language} code with {selected_framework}..."):
                generated_code = generate_code(code_prompt, selected_language, selected_framework)
                st.session_state.generated_code = generated_code
        else:
            st.error("Please enter a description of what you want to build.")

    # Generated Code Section - Full Width
    st.markdown("---")
    st.subheader("Generated Code")
    if 'generated_code' in st.session_state:
        st.code(st.session_state.generated_code, language="python")
        st.download_button(
            label="ğŸ“¥ Download Code",
            data=st.session_state.generated_code,
            file_name=f"generated_code.{selected_language.lower()}",
            mime="text/plain"
        )
    else:
        st.info("Generated code will appear here.")

# 3. Test Case Generator
elif page == "ğŸ§ª Test Generator":
    st.header("ğŸ§ª Test Case Generator")
    st.markdown("*Automatically create unit tests or functional tests from code or requirements*")

    col1, col2 = st.columns([1, 1])

    with col1:
        st.subheader("Test Generation Settings")

        # Test type selection
        test_types = ["Unit Tests", "Integration Tests", "Functional Tests", "API Tests", "End-to-End Tests"]
        selected_test_type = st.selectbox("Select Test Type:", test_types)

        # Language selection
        test_languages = ["Python", "JavaScript", "Java", "C#", "Go", "TypeScript", "PHP", "Ruby"]
        selected_test_language = st.selectbox("Select Language:", test_languages)

        # Code input for testing
        code_to_test = st.text_area(
            "Enter Code to Generate Tests For:",
            placeholder="Paste your code here to generate tests...",
            height=200
        )

        # Alternative: requirements-based test generation
        requirements_for_tests = st.text_area(
            "Or Enter Requirements for Test Generation:",
            placeholder="Describe functionality to generate tests for...",
            height=100
        )

        if st.button("ğŸ§ª Generate Tests", type="primary"):
            if code_to_test or requirements_for_tests:
                with st.spinner(f"ğŸ¤– Generating {selected_test_type}..."):
                    if code_to_test:
                        test_code = generate_tests(code_to_test, selected_test_type, selected_test_language)
                    else:
                        test_code = generate_tests(requirements_for_tests, selected_test_type, selected_test_language)
                    st.session_state.generated_tests = test_code
            else:
                st.error("Please provide code or requirements to generate tests for.")

    with col2:
        st.subheader("Generated Tests")
        if 'generated_tests' in st.session_state:
            st.code(st.session_state.generated_tests, language="python")
            st.download_button(
                label="ğŸ“¥ Download Tests",
                data=st.session_state.generated_tests,
                file_name=f"generated_tests.{selected_test_language.lower()}",
                mime="text/plain"
            )
        else:
            st.info("Generated tests will appear here.")

# 4. AI Assistant
elif page == "ğŸ¤– AI Assistant":
    st.header("ğŸ¤– AI Development Assistant")
    st.markdown("*Your expert AI companion for all software development questions*")

    # Initialize chat history
    if 'chat_history' not in st.session_state:
        st.session_state.chat_history = []

    # Chat container
    chat_container = st.container()
    with chat_container:
        for message in st.session_state.chat_history:
            if message["role"] == "user":
                st.markdown(f"**ğŸ‘¤ You:** {message['content']}")
            else:
                st.markdown(f"**ğŸ¤– Assistant:** {message['content']}")

    st.markdown("---")

    # Suggested questions
    st.subheader("ğŸ’¡ Suggested Questions:")
    suggested_questions = [
        "What are the best practices for API design?",
        "How to implement microservices architecture?",
        "What testing strategy should I use for my project?",
        "How to optimize database queries?",
        "What are the differences between REST and GraphQL?",
        "How to implement CI/CD pipeline?",
        "What security best practices should I follow?",
        "How to choose the right database for my project?"
    ]

    cols = st.columns(4)
    for i, question in enumerate(suggested_questions):
        if cols[i % 4].button(question, key=f"suggest_{i}"):
            st.session_state.temp_input = question

    # Chat input
    user_input = st.text_input(
        "Ask me about software development, SDLC, or any technical questions:",
        value=st.session_state.get('temp_input', ''),
        key="chat_input"
    )

    col1, col2 = st.columns([1, 4])
    with col1:
        send_button = st.button("ğŸ’¬ Send Message", type="primary")
    with col2:
        if st.button("ğŸ—‘ï¸ Clear Chat"):
            st.session_state.chat_history = []
            st.rerun()

    if send_button and user_input:
        # Add user message to history
        st.session_state.chat_history.append({"role": "user", "content": user_input})

        with st.spinner("ğŸ¤– Thinking..."):
            # Get context from recent messages
            context = "\\n".join([
                f"{msg['role']}: {msg['content']}"
                for msg in st.session_state.chat_history[-3:]
            ])

            # Generate response
            response = chat_assistant(user_input, context)
            st.session_state.chat_history.append({"role": "assistant", "content": response})

        # Clear temp input
        if 'temp_input' in st.session_state:
            del st.session_state.temp_input

        st.rerun()

# Sidebar information
st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ“‹ Features:")
st.sidebar.markdown("""
- **ğŸ“„ Requirement Analysis**: Extract requirements from PDFs/text
- **ğŸ’» Code Generator**: Generate code with language/framework selection
- **ğŸ§ª Test Generator**: Create comprehensive test suites
- **ğŸ¤– AI Assistant**: Expert development guidance
""")

st.sidebar.markdown("---")
st.sidebar.markdown("### â„¹ï¸ MODEL Info:")
st.sidebar.markdown("**Model:** ibm-granite/granite-3.3-2b-instruct")
st.sidebar.markdown("**Status:** ğŸŸ¢ Connected")

st.sidebar.markdown("---")
st.sidebar.markdown("### ğŸ›¡ï¸ Privacy:")
st.sidebar.markdown("""
- IBM Model Successfully Integrated
- No data is stored permanently
- Sessions are isolated
""")

# Footer
st.markdown("---")
st.markdown("*Built with Streamlit and IBM Granite Model - Comprehensive Software Development Tool*")
''')

print("âœ… SmartSDLC app with IBM Granite model created successfully!")

# --- 2. Start Streamlit with ngrok ---
from pyngrok import ngrok
import subprocess
import threading
import time
import os

# Get ngrok token
NGROK_TOKEN = os.getenv('NGROK_AUTH_TOKEN', '2yiaK4duX6yWfRYE1de9PdNmKH8_4VCqUMchZ7i5RErPgf3dT')

try:
    ngrok.set_auth_token(NGROK_TOKEN)
    print("âœ… Ngrok token set successfully")
except Exception as e:
    print(f"âŒ Error setting ngrok token: {e}")

def run_streamlit():
    try:
        subprocess.run([
            "streamlit", "run", "smartsdlc_app.py",
            "--server.port", "8501",
            "--server.headless", "true",
            "--server.address", "0.0.0.0"
        ])
    except Exception as e:
        print(f"Error running Streamlit: {e}")

# Start Streamlit
print("ğŸš€ Starting SmartSDLC app with IBM Granite model...")
streamlit_thread = threading.Thread(target=run_streamlit, daemon=True)
streamlit_thread.start()

# Wait for Streamlit to start
print("â³ Waiting for Streamlit to initialize...")
time.sleep(15)

# Create ngrok tunnel
try:
    public_url = ngrok.connect(8501)
    print(f"\\nâœ… SmartSDLC with IBM Granite is now running!")
    print(f"ğŸŒ Public URL: {public_url}")
    print(f"ğŸ”— Click the link above to access your SmartSDLC app!")
    print(f"ğŸ“± The app is mobile-friendly and works on all devices")
    print("---")
    print("ğŸ› ï¸  All Features Available:")
    print("   â€¢ ğŸ“„ PDF/Prompt-Based Requirement Analysis")
    print("   â€¢ ğŸ’» AI Code Generator (Multiple Languages & Frameworks)")
    print("   â€¢ ğŸ§ª Test Case Generator (Unit, Integration, Functional)")
    print("   â€¢ ğŸ¤– AI Development Assistant")
    print("---")
    print("ğŸ”‘ Using IBM Granite Model (Local - No API needed)")
    print("âš¡ Fast responses with local model inference")
    print("â¹ï¸  Press Ctrl+C to stop the application")

    # Keep the app running
    try:
        while True:
            time.sleep(1)
    except KeyboardInterrupt:
        print("\\nğŸ›‘ Stopping the app...")
        ngrok.disconnect(public_url)
        ngrok.kill()
        print("âœ… App stopped successfully")

except Exception as e:
    print(f"âŒ Error starting ngrok tunnel: {e}")
    print("Possible solutions:")
    print("1. Check your internet connection")
    print("2. Verify your ngrok auth token")
    print("3. Make sure port 8501 is not already in use")
    print("4. Try restarting the kernel and running again")